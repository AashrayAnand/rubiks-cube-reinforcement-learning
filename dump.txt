====CURR STATE========
======================
random value generated is 0.4492078199757945
FOLLOWING POLICY
q value for action front from curr state is -1.084878764544
q value for action back from curr state is -1.095925996032
q value for action left from curr state is -0.899550764544
q value for action right from curr state is -0.899550764544
q value for action top from curr state is -0.899550764544
q value for action bottom from curr state is -0.597357433344
actions chosen = bottom
new q value for bottom action is -0.5971429733376
=====EPISODE 7=====
====CURR STATE========
======================
random value generated is 0.1094476926150959
=====EPISODE 8=====
====CURR STATE========
======================
random value generated is 0.39556162408336015
FOLLOWING POLICY
q value for action front from curr state is -1.0921515058176
q value for action back from curr state is -1.0965703984128
q value for action left from curr state is -0.8992203058176
q value for action right from curr state is -0.8992203058176
q value for action top from curr state is -0.8992203058176
q value for action bottom from curr state is -0.5971429733376
actions chosen = left
new q value for left action is -0.89908812232704
=====EPISODE 9=====
====CURR STATE========
======================
random value generated is 0.7884770316912186
FOLLOWING POLICY
q value for action front from curr state is -0.90716
q value for action back from curr state is -0.90716
q value for action left from curr state is -0.18148000000000009
q value for action right from curr state is -0.90716
q value for action top from curr state is -0.48716000000000004
q value for action bottom from curr state is -0.60716
actions chosen = top
new q value for top action is -0.314264
======= ROUND 41=========
executing front 180 rotation
executing top 180 rotation
executing right 180 rotation
=====EPISODE 0=====
====CURR STATE========
======================
random value generated is 0.42431940146107383
FOLLOWING POLICY
q value for action front from curr state is -3
q value for action back from curr state is -3
q value for action left from curr state is -3
q value for action right from curr state is 3
q value for action top from curr state is -3
q value for action bottom from curr state is -3
actions chosen = right
new q value for right action is 0.8418000000000001
=====EPISODE 1=====
====CURR STATE========
======================
random value generated is 0.3023343713577764
FOLLOWING POLICY
q value for action front from curr state is -0.82148
q value for action back from curr state is -1.2414800000000001
q value for action left from curr state is -1.2414800000000001
q value for action right from curr state is -1.2414800000000001
q value for action top from curr state is 83.228
q value for action bottom from curr state is -1.2414800000000001
actions chosen = front

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'G'], ['B', 'B', 'G'], ['B', 'B', 'G']]
RIGHT[['B', 'G', 'G'], ['B', 'G', 'G'], ['B', 'G', 'G']]
TOP[['O', 'O', 'O'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['R', 'R', 'R'], ['O', 'O', 'O'], ['O', 'O', 'O']]

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
REWARD IS GOAL
new q value for front action is -0.6867920000000001
=====EPISODE 2=====
====CURR STATE========
======================
random value generated is 0.8421610391173057
FOLLOWING POLICY
q value for action front from curr state is 3
q value for action back from curr state is -3
q value for action left from curr state is -3
q value for action right from curr state is -3
q value for action top from curr state is -3
q value for action bottom from curr state is -3
actions chosen = back
new q value for back action is -1.7394
=====EPISODE 3=====
====CURR STATE========
======================
random value generated is 0.11685316456404604
=====EPISODE 4=====
====CURR STATE========
======================
random value generated is 0.7957955962348593
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = front
new q value for front action is -0.6582
=====EPISODE 5=====
====CURR STATE========
======================
random value generated is 0.8428709451860026
FOLLOWING POLICY
q value for action front from curr state is -2
q value for action back from curr state is -2
q value for action left from curr state is -2
q value for action right from curr state is -2
q value for action top from curr state is 2
q value for action bottom from curr state is -2
actions chosen = top
new q value for top action is -1.0581999999999998
=====EPISODE 6=====
====CURR STATE========
======================
random value generated is 0.1723050988548066
FOLLOWING POLICY
q value for action front from curr state is -0.021479999999999944
q value for action back from curr state is -0.9814800000000001
q value for action left from curr state is -0.81516
q value for action right from curr state is -0.81516
q value for action top from curr state is -0.81516
q value for action bottom from curr state is -1.23516
actions chosen = left
new q value for left action is -0.565464
=====EPISODE 7=====
====CURR STATE========
======================
random value generated is 0.06015606325576084
=====EPISODE 8=====
====CURR STATE========
======================
random value generated is 0.5101415738287571
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = back
new q value for back action is -0.1194
=====EPISODE 9=====
====CURR STATE========
======================
random value generated is 0.38355724745861597
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = bottom
new q value for bottom action is -0.1194
======= ROUND 42=========
executing top 180 rotation
executing front 180 rotation
executing back 180 rotation
=====EPISODE 0=====
====CURR STATE========
======================
random value generated is 0.4552546153289324
FOLLOWING POLICY
q value for action front from curr state is 3
q value for action back from curr state is -3
q value for action left from curr state is -3
q value for action right from curr state is -3
q value for action top from curr state is -3
q value for action bottom from curr state is -3
actions chosen = front
new q value for front action is 0.8418000000000001
=====EPISODE 1=====
====CURR STATE========
======================
random value generated is 0.3643388285052921
FOLLOWING POLICY
q value for action front from curr state is -1.2414800000000001
q value for action back from curr state is 83.228
q value for action left from curr state is -1.2414800000000001
q value for action right from curr state is -1.2414800000000001
q value for action top from curr state is -0.82148
q value for action bottom from curr state is -1.2414800000000001
actions chosen = back

FRONT[['Y', 'Y', 'Y'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['W', 'W', 'W'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['G', 'G', 'G'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['B', 'B', 'B'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
REWARD IS GOAL
new q value for back action is 92.5112
=====EPISODE 2=====
====CURR STATE========
======================
random value generated is 0.2673700420549453
FOLLOWING POLICY
q value for action front from curr state is -3.0969999833355266
q value for action back from curr state is -3.0969999833355266
q value for action left from curr state is -3.0969999833355266
q value for action right from curr state is -3.0969999833355266
q value for action top from curr state is 97.22798381175286
q value for action bottom from curr state is -1.0970003269329105
actions chosen = top

FRONT[['Y', 'Y', 'Y'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['W', 'W', 'W'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['G', 'G', 'G'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['B', 'B', 'B'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
REWARD IS GOAL
new q value for top action is 97.22799352470115
reached goal state while in Q-learning epsiode 2
======= ROUND 43=========
executing top 180 rotation
executing bottom 180 rotation
executing left 180 rotation
=====EPISODE 0=====
====CURR STATE========
======================
random value generated is 0.26452386441255016
FOLLOWING POLICY
q value for action front from curr state is -0.81516
q value for action back from curr state is -0.81516
q value for action left from curr state is -0.021479999999999944
q value for action right from curr state is -1.23516
q value for action top from curr state is -0.81516
q value for action bottom from curr state is -0.81516
actions chosen = left
new q value for left action is -0.366792
=====EPISODE 1=====
====CURR STATE========
======================
random value generated is 0.14743191526964727
FOLLOWING POLICY
q value for action front from curr state is -3.096998822105219
q value for action back from curr state is -3.096998822105219
q value for action left from curr state is -3.096998822105219
q value for action right from curr state is -3.096998822105219
q value for action top from curr state is 98.69989616916563
q value for action bottom from curr state is 98.69989187419833
actions chosen = top

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['Y', 'Y', 'Y']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['W', 'W', 'W']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['G', 'G', 'G']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['B', 'B', 'B']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
REWARD IS GOAL

FRONT[['Y', 'Y', 'Y'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['W', 'W', 'W'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['G', 'G', 'G'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['B', 'B', 'B'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
REWARD IS GOAL
new q value for top action is 98.69995846766625
=====EPISODE 2=====
====CURR STATE========
======================
random value generated is 0.14720710430027917
FOLLOWING POLICY
q value for action front from curr state is -3.096602688
q value for action back from curr state is -3.096602688
q value for action left from curr state is -3.096602688
q value for action right from curr state is -3.096602688
q value for action top from curr state is -1.104794688
q value for action bottom from curr state is 96.84204211199999
actions chosen = bottom

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['Y', 'Y', 'Y']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['W', 'W', 'W']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['G', 'G', 'G']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['B', 'B', 'B']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
REWARD IS GOAL
new q value for bottom action is 97.07361684479999
reached goal state while in Q-learning epsiode 2
======= ROUND 44=========
executing top 180 rotation
executing bottom 180 rotation
executing right 180 rotation
=====EPISODE 0=====
====CURR STATE========
======================
random value generated is 0.14800305081538745
FOLLOWING POLICY
q value for action front from curr state is -0.40070459136000003
q value for action back from curr state is -0.40070459136000003
q value for action left from curr state is -0.90037691136
q value for action right from curr state is -0.5946426700800002
q value for action top from curr state is -0.40070459136000003
q value for action bottom from curr state is -0.40070459136000003
actions chosen = front
new q value for front action is -0.399681836544
=====EPISODE 1=====
====CURR STATE========
======================
random value generated is 0.45867821419075616
FOLLOWING POLICY
q value for action front from curr state is 0.44179999999999997
q value for action back from curr state is -0.9194
q value for action left from curr state is -1.2193999999999998
q value for action right from curr state is -0.9194
q value for action top from curr state is -0.9194
q value for action bottom from curr state is -0.9194
actions chosen = back
new q value for back action is -0.48716000000000004
=====EPISODE 2=====
====CURR STATE========
======================
random value generated is 0.8884006707989409
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = front
new q value for front action is -0.2394
=====EPISODE 3=====
====CURR STATE========
======================
random value generated is 0.311319777368946
FOLLOWING POLICY
q value for action front from curr state is -2
q value for action back from curr state is 2
q value for action left from curr state is -2
q value for action right from curr state is -2
q value for action top from curr state is -2
q value for action bottom from curr state is -2
actions chosen = left
new q value for left action is -1.2193999999999998
=====EPISODE 4=====
====CURR STATE========
======================
random value generated is 0.0513356232349198
=====EPISODE 5=====
====CURR STATE========
======================
random value generated is 0.7424494178300406
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = left
new q value for left action is -0.1194
=====EPISODE 6=====
====CURR STATE========
======================
random value generated is 0.7199691063412103
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = top
new q value for top action is -0.1194
=====EPISODE 7=====
====CURR STATE========
======================
random value generated is 0.11636128956279679
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = back
new q value for back action is -0.1194
=====EPISODE 8=====
====CURR STATE========
======================
random value generated is 0.7618627308401529
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = right
new q value for right action is -0.1194
=====EPISODE 9=====
====CURR STATE========
======================
random value generated is 0.6880812478359241
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = left
new q value for left action is -1.6194000000000002
======= ROUND 45=========
executing top 180 rotation
executing right 180 rotation
executing back 180 rotation
=====EPISODE 0=====
====CURR STATE========
======================
random value generated is 0.2947982878391767
FOLLOWING POLICY
q value for action front from curr state is -3
q value for action back from curr state is 3
q value for action left from curr state is -3
q value for action right from curr state is -3
q value for action top from curr state is -3
q value for action bottom from curr state is -3
actions chosen = back
new q value for back action is 0.8418000000000001
=====EPISODE 1=====
====CURR STATE========
======================
random value generated is 0.9349369079843964
FOLLOWING POLICY
q value for action front from curr state is -1.1201168
q value for action back from curr state is -1.1201168
q value for action left from curr state is -1.1201168
q value for action right from curr state is 96.22448
q value for action top from curr state is -0.6329168000000001
q value for action bottom from curr state is -1.1201168
actions chosen = right

FRONT[['Y', 'Y', 'Y'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['W', 'W', 'W'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['G', 'G', 'G'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['B', 'B', 'B'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
REWARD IS GOAL
new q value for right action is 97.70979200000001
=====EPISODE 2=====
====CURR STATE========
======================
random value generated is 0.8639696742837276
FOLLOWING POLICY
q value for action front from curr state is -3.0969999933342107
q value for action back from curr state is -3.0969999933342107
q value for action left from curr state is -3.0969999933342107
q value for action right from curr state is -3.0969999933342107
q value for action top from curr state is 97.22799352470115
q value for action bottom from curr state is -1.0970001307731643
actions chosen = top

FRONT[['Y', 'Y', 'Y'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['W', 'W', 'W'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['G', 'G', 'G'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['B', 'B', 'B'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
REWARD IS GOAL
new q value for top action is 97.22799740988046
reached goal state while in Q-learning epsiode 2
======= ROUND 46=========
executing back 180 rotation
executing left 180 rotation
executing right 180 rotation
=====EPISODE 0=====
====CURR STATE========
======================
random value generated is 0.041719918615089946
=====EPISODE 1=====
====CURR STATE========
======================
random value generated is 0.7635205058630908
FOLLOWING POLICY
q value for action front from curr state is -2
q value for action back from curr state is -2
q value for action left from curr state is -2
q value for action right from curr state is -2
q value for action top from curr state is -2
q value for action bottom from curr state is 2
actions chosen = front
new q value for front action is -0.9194
=====EPISODE 2=====
====CURR STATE========
======================
random value generated is 0.9155866372432193
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = right
new q value for right action is -0.1194
=====EPISODE 3=====
====CURR STATE========
======================
random value generated is 0.4746122578611234
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = back
new q value for back action is -0.41939999999999994
=====EPISODE 4=====
====CURR STATE========
======================
random value generated is 0.18123911280069394
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = top
new q value for top action is -0.41939999999999994
=====EPISODE 5=====
====CURR STATE========
======================
random value generated is 0.8374777566111365
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = front
new q value for front action is -0.1194
=====EPISODE 6=====
====CURR STATE========
======================
random value generated is 0.6394278226664467
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = top
new q value for top action is -0.1194
=====EPISODE 7=====
====CURR STATE========
======================
random value generated is 0.5134507591205725
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = back
new q value for back action is -0.1194
=====EPISODE 8=====
====CURR STATE========
======================
random value generated is 0.7557764330181325
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = right
new q value for right action is -0.41939999999999994
=====EPISODE 9=====
====CURR STATE========
======================
random value generated is 0.10599415144334079
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = back
new q value for back action is -0.1194
======= ROUND 47=========
executing top 180 rotation
executing right 180 rotation
executing right 180 rotation
=====EPISODE 0=====
====CURR STATE========
======================
random value generated is 0.5232309732131597
FOLLOWING POLICY
q value for action front from curr state is -3.0969999973336844
q value for action back from curr state is -3.0969999973336844
q value for action left from curr state is -3.0969999973336844
q value for action right from curr state is -3.0969999973336844
q value for action top from curr state is 97.22799740988046
q value for action bottom from curr state is -1.0970000523092658
actions chosen = top

FRONT[['Y', 'Y', 'Y'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['W', 'W', 'W'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['G', 'G', 'G'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['B', 'B', 'B'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
REWARD IS GOAL
new q value for top action is 97.22799896395217
reached goal state while in Q-learning epsiode 0
======= ROUND 48=========
executing right 180 rotation
executing back 180 rotation
executing left 180 rotation
=====EPISODE 0=====
====CURR STATE========
======================
random value generated is 0.12763765515676384
FOLLOWING POLICY
q value for action front from curr state is -3
q value for action back from curr state is -3
q value for action left from curr state is 3
q value for action right from curr state is -3
q value for action top from curr state is -3
q value for action bottom from curr state is -3
actions chosen = left
new q value for left action is 0.8418000000000001
=====EPISODE 1=====
====CURR STATE========
======================
random value generated is 0.23909317417009046
FOLLOWING POLICY
q value for action front from curr state is -1.100698688
q value for action back from curr state is 98.30391680000001
q value for action left from curr state is -1.100698688
q value for action right from curr state is -1.095719168
q value for action top from curr state is -1.100698688
q value for action bottom from curr state is -1.100698688
actions chosen = right

FRONT[['W', 'W', 'Y'], ['W', 'W', 'Y'], ['W', 'W', 'Y']]
BACK[['W', 'Y', 'Y'], ['W', 'Y', 'Y'], ['W', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['O', 'R', 'R'], ['O', 'R', 'R'], ['O', 'R', 'R']]
BOTTOM[['O', 'O', 'R'], ['O', 'O', 'R'], ['O', 'O', 'R']]

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
REWARD IS GOAL
new q value for right action is -1.0934876672
=====EPISODE 2=====
====CURR STATE========
======================
random value generated is 0.3870282413148576
FOLLOWING POLICY
q value for action front from curr state is -1.7394
q value for action back from curr state is -1.7394
q value for action left from curr state is -1.7394
q value for action right from curr state is 0.5418000000000003
q value for action top from curr state is -1.8582
q value for action bottom from curr state is -1.8582
actions chosen = front
new q value for front action is -1.23516
=====EPISODE 3=====
====CURR STATE========
======================
random value generated is 0.7123516246191977
FOLLOWING POLICY
q value for action front from curr state is 2
q value for action back from curr state is -2
q value for action left from curr state is -2
q value for action right from curr state is -2
q value for action top from curr state is -2
q value for action bottom from curr state is -2
actions chosen = back
new q value for back action is -1.2193999999999998
=====EPISODE 4=====
====CURR STATE========
======================
random value generated is 0.6430115638229448
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = bottom
new q value for bottom action is -0.41939999999999994
=====EPISODE 5=====
====CURR STATE========
======================
random value generated is 0.13434902361002143
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = top
new q value for top action is -0.41939999999999994
=====EPISODE 6=====
====CURR STATE========
======================
random value generated is 0.5672873253088245
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = front
new q value for front action is -0.1194
=====EPISODE 7=====
====CURR STATE========
======================
random value generated is 0.3310506352779359
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = bottom
new q value for bottom action is -0.1194
=====EPISODE 8=====
====CURR STATE========
======================
random value generated is 0.5559803845893957
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = front
new q value for front action is -0.1194
=====EPISODE 9=====
====CURR STATE========
======================
random value generated is 0.15083664455193102
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = left
new q value for left action is -0.41939999999999994
======= ROUND 49=========
executing front 180 rotation
executing right 180 rotation
executing front 180 rotation
=====EPISODE 0=====
====CURR STATE========
======================
random value generated is 0.09166744347780365
FOLLOWING POLICY
q value for action front from curr state is 3
q value for action back from curr state is -3
q value for action left from curr state is -3
q value for action right from curr state is -3
q value for action top from curr state is -3
q value for action bottom from curr state is -3
actions chosen = back
new q value for back action is -1.8582
=====EPISODE 1=====
====CURR STATE========
======================
random value generated is 0.6469155001827764
FOLLOWING POLICY
q value for action front from curr state is 2
q value for action back from curr state is -2
q value for action left from curr state is -2
q value for action right from curr state is -2
q value for action top from curr state is -2
q value for action bottom from curr state is -2
actions chosen = front
new q value for front action is 0.14180000000000015
=====EPISODE 2=====
====CURR STATE========
======================
random value generated is 0.7356917998182726
FOLLOWING POLICY
q value for action front from curr state is -3
q value for action back from curr state is 3
q value for action left from curr state is -3
q value for action right from curr state is -3
q value for action top from curr state is -3
q value for action bottom from curr state is -3
actions chosen = left
new q value for left action is -1.4394
=====EPISODE 3=====
====CURR STATE========
======================
random value generated is 0.33608082896295544
FOLLOWING POLICY
q value for action front from curr state is -2
q value for action back from curr state is -2
q value for action left from curr state is 2
q value for action right from curr state is -2
q value for action top from curr state is -2
q value for action bottom from curr state is -2
actions chosen = back
new q value for back action is -0.9194
=====EPISODE 4=====
====CURR STATE========
======================
random value generated is 0.32481112807961454
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = left
new q value for left action is -0.41939999999999994
=====EPISODE 5=====
====CURR STATE========
======================
random value generated is 0.7090732100402398
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = top
new q value for top action is -1.6194000000000002
=====EPISODE 6=====
====CURR STATE========
======================
random value generated is 0.3953866049950798
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = back
new q value for back action is -0.1194
=====EPISODE 7=====
====CURR STATE========
======================
random value generated is 0.12546188828552007
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = front
new q value for front action is -0.1194
=====EPISODE 8=====
====CURR STATE========
======================
random value generated is 0.24211834995167325
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = top
new q value for top action is -0.1194
=====EPISODE 9=====
====CURR STATE========
======================
random value generated is 0.20091319838996013
FOLLOWING POLICY
q value for action front from curr state is 0
q value for action back from curr state is 0
q value for action left from curr state is 0
q value for action right from curr state is 0
q value for action top from curr state is 0
q value for action bottom from curr state is 0
actions chosen = right
new q value for right action is -0.41939999999999994
there are 5226 keys in Q Table
actions chosen = back
last action = None
q value is -0.021479999999999944

FRONT[['Y', 'W', 'Y'], ['Y', 'W', 'Y'], ['Y', 'W', 'Y']]
BACK[['W', 'Y', 'W'], ['W', 'Y', 'W'], ['W', 'Y', 'W']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['O', 'R', 'O'], ['O', 'R', 'O'], ['O', 'R', 'O']]
BOTTOM[['R', 'O', 'R'], ['R', 'O', 'R'], ['R', 'O', 'R']]
actions chosen = left
last action = back
q value is 92.5112

FRONT[['W', 'W', 'Y'], ['W', 'W', 'Y'], ['W', 'W', 'Y']]
BACK[['W', 'Y', 'Y'], ['W', 'Y', 'Y'], ['W', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['O', 'R', 'R'], ['O', 'R', 'R'], ['O', 'R', 'R']]
BOTTOM[['O', 'O', 'R'], ['O', 'O', 'R'], ['O', 'O', 'R']]
actions chosen = right
last action = left
q value is 97.20329869516799

FRONT[['W', 'W', 'W'], ['W', 'W', 'W'], ['W', 'W', 'W']]
BACK[['Y', 'Y', 'Y'], ['Y', 'Y', 'Y'], ['Y', 'Y', 'Y']]
LEFT[['B', 'B', 'B'], ['B', 'B', 'B'], ['B', 'B', 'B']]
RIGHT[['G', 'G', 'G'], ['G', 'G', 'G'], ['G', 'G', 'G']]
TOP[['R', 'R', 'R'], ['R', 'R', 'R'], ['R', 'R', 'R']]
BOTTOM[['O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'O']]
AGENT REACHED A GOAL STATE!!!
=============
number of q values in dictionary is 5226
number of q values with zero value is 0
number of q value with non zero value is 5226
number of re visited states = 575
{'front': 162, 'back': 142, 'left': 122, 'right': 123, 'top': 119, 'bottom': 105}
D-10-18-189-127:a6 aashray$